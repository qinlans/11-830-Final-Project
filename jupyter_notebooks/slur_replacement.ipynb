{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from tqdm import tqdm, tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = '../data/'\n",
    "fname = '{}hatebase_slurs.txt'.format(path)\n",
    "#fname = '{}hatebase+zeerak_exclude_slurs.txt'.format(path)\n",
    "slurs = pd.read_csv(fname, header=None)[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = '../data/davidson/'\n",
    "# path = '../data/zeerak_naacl/'\n",
    "debug = pd.read_csv('{}debug.csv'.format(path), encoding='utf-8')\n",
    "train = pd.read_csv('{}train.csv'.format(path), encoding='utf-8')\n",
    "dev = pd.read_csv('{}dev.csv'.format(path), encoding='utf-8')\n",
    "test = pd.read_csv('{}test.csv'.format(path), encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_join(word_list):\n",
    "    joined = ''\n",
    "    last_word = None\n",
    "    for i, w in enumerate(word_list):\n",
    "        if last_word == '<' or w == '>' or i == 0:\n",
    "            joined += w\n",
    "        else:\n",
    "            joined += ' ' + w\n",
    "        last_word = w\n",
    "    return joined\n",
    "\n",
    "def pos_replace(tweet):\n",
    "    \"\"\"Repalce slurs with their POS tags.\"\"\"\n",
    "    tokenized_tweet = word_tokenize(tweet)\n",
    "    pos_tweet = pos_tag(tokenized_tweet)\n",
    "    pos_replaced = [t if t not in slurs else pos for (t, pos) in pos_tweet]\n",
    "    return safe_join(pos_replaced)\n",
    "\n",
    "def unk_replace(tweet):\n",
    "    \"\"\"Replace slurs with <UNK>.\"\"\"\n",
    "    tokenized_tweet = word_tokenize(tweet)\n",
    "    unk_replaced = [t if t not in slurs else '<UNK>' for t in tokenized_tweet]\n",
    "    return safe_join(unk_replaced)\n",
    "\n",
    "def slur_remove(tweet):\n",
    "    \"\"\"Remove slurs from the sentence.\"\"\"\n",
    "    tokenized_tweet = word_tokenize(tweet)\n",
    "    slur_removed = [t for t in tokenized_tweet if t not in slurs ]\n",
    "    return safe_join(slur_removed)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train, length: 19856\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "367648083870432289c5cac8108f8bce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing test, length: 2464\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04da511236e049e5a76be8506c37f5f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing dev, length: 2463\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f68d1079d674a228f39bf12104d8770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_datasets = {'train': train, 'test': test, 'dev': dev}\n",
    "new_cols = ['tweet_unk_slur', 'tweet_no_slur', 'tweet_pos_slur']\n",
    "\n",
    "for k, d in all_datasets.items():\n",
    "    tqdm.write('Processing {}, length: {}'.format(k, len(d)))\n",
    "    tweets = d['tweet'].values\n",
    "    proc_tweets = []\n",
    "    for c in new_cols:\n",
    "        d[c] = ''\n",
    "    for i, t in enumerate(tqdm_notebook(tweets)):\n",
    "        proc_tweets.append( [unk_replace(t), slur_remove(t), pos_replace(t)])\n",
    "    d[new_cols] = proc_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.to_csv('{}train.csv'.format(path), index=None, encoding='utf-8')\n",
    "dev.to_csv('{}dev.csv'.format(path), index=None, encoding='utf-8')\n",
    "test.to_csv('{}test.csv'.format(path), index=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
