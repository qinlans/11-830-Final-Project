{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "# Local imports\n",
    "from preprocessing import clean_tweets, one_hot_encode, make_debug_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First load and clean the hatebase terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Clean the hatebase slurs\n",
    "def clean_slurs(text):\n",
    "    \"\"\"Lowercase and underscore join slur words.\"\"\"\n",
    "    return text.strip().lower().replace(' ', '_')\n",
    "\n",
    "slurs = pd.read_csv('../data/original_hatebase_slurs.txt', header=None)\n",
    "\n",
    "# Clean slurs\n",
    "slur_list = [*map(lambda s: s.lower(), slurs[0].values)]\n",
    "cleaned_slurs = [*map(clean_slurs, slur_list)]\n",
    "pluralize_slurs = [s + end for s in cleaned_slurs for end in ['s', 'es']]\n",
    "full_slur_list = sorted(pluralize_slurs + cleaned_slurs)\n",
    "\n",
    "# Outputs\n",
    "slur_map = {s: cs for s, cs in zip(slur_list, cleaned_slurs) if s != cs}\n",
    "#out_slurs = pd.DataFrame(full_slur_list)\n",
    "#out_slurs.to_csv('data/hatebase_slurs.txt', index=None, header=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_slurs_in_context(text):\n",
    "    \"\"\"Replace slurs with their cleaned versions.\"\"\"\n",
    "    for k, v in slur_map.items():\n",
    "        text = text.replace(k, v)\n",
    "    return text\n",
    "\n",
    "def extract_slurs(text):\n",
    "    \"\"\"Get a list of all slurs used in the text.\"\"\"\n",
    "    text = text.split(' ')\n",
    "    all_slurs = []\n",
    "    for s in full_slur_list:\n",
    "        if s in text:\n",
    "            all_slurs += [s]\n",
    "    return all_slurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Davidson et al data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = '../data/davidson/'\n",
    "fname = '{}labeled_data.csv'.format(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv(fname, encoding='utf-8', index_col='Unnamed: 0').sample(frac=1)\n",
    "label_map = {0: 'hate_speech', 1: 'offensive_language', 2: 'neither'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clean the tweets\n",
    "df = clean_tweets(df)\n",
    "\n",
    "# Convert columns to one hot encoding\n",
    "df[['hate_speech', 'offensive_language', 'neither']] = \\\n",
    "    one_hot_encode(df['class'])\n",
    "df['label'] =df['class'].apply(lambda c: label_map[c])\n",
    "    \n",
    "# Clean hate speech terms, and extract slurs\n",
    "df['tweet'] = df['tweet'].apply(clean_slurs_in_context)\n",
    "df['slurs'] = df['tweet'].apply(extract_slurs)\n",
    "    \n",
    "# Re-order the DataFrame, and drop some columns\n",
    "df = df[['tweet', 'label', 'mentions', 'hashtags', 'slurs', 'original_tweet',\n",
    "         'hate_speech', 'offensive_language', 'neither']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make a test/dev/train split\n",
    "train_perc = 0.80\n",
    "msk = np.random.rand(len(df)) < train_perc\n",
    "train = df[msk]\n",
    "not_train = df[~msk]\n",
    "half = int(len(not_train) / 2)\n",
    "dev = not_train[:half]\n",
    "test = not_train[half:]\n",
    "\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "debug = make_debug_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "debug.to_csv('{}debug.csv'.format(path), index=False, encoding='utf-8')\n",
    "train.to_csv('{}train.csv'.format(path), index=False, encoding='utf-8')\n",
    "dev.to_csv('{}dev.csv'.format(path), index=False, encoding='utf-8')\n",
    "test.to_csv('{}test.csv'.format(path), index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Zeerak data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = '../data/zeerak_naacl/'\n",
    "fname = '{}zeerak_naacl_tweets.csv'.format(path)\n",
    "# Load the data\n",
    "df2 = pd.read_csv(fname, encoding='utf-8').sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First mask out some missing data\n",
    "msk = ~ df2['label'].apply(lambda t: type(t) is float)\n",
    "df2 = df2[msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Clean the tweets\n",
    "df2.rename(index=str, columns={'text': 'tweet'}, inplace=True)\n",
    "df2 = clean_tweets(df2)\n",
    "\n",
    "# Label cleanup to match the other df format\n",
    "labels = ['racism', 'sexism', 'none']\n",
    "one_hot_label = [labels.index(l) for l in df2['label']]\n",
    "for l in labels:\n",
    "    df2[l] = -1\n",
    "\n",
    "# Convert columns to one hot encoding\n",
    "df2[['racism', 'sexism', 'none']] = \\\n",
    "    one_hot_encode(one_hot_label)\n",
    "    \n",
    "# Clean hate speech terms, and extract slurs\n",
    "df2['tweet'] = df2['tweet'].apply(clean_slurs_in_context)\n",
    "df2['slurs'] = df2['tweet'].apply(extract_slurs)\n",
    "    \n",
    "# Re-order the DataFrame, and drop some columns\n",
    "df2 = df2[['tweet', 'label', 'mentions', 'hashtags', 'slurs', 'original_tweet',\n",
    "           'racism', 'sexism', 'none', 'tweet_id', 'user_screen_name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make a test/dev/train split\n",
    "train_perc = 0.80\n",
    "msk = np.random.rand(len(df2)) < train_perc\n",
    "train = df2[msk]\n",
    "not_train = df2[~msk]\n",
    "half = int(len(not_train) / 2)\n",
    "dev = not_train[:half]\n",
    "test = not_train[half:]\n",
    "\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "debug = make_debug_df(df2, cols=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "debug.to_csv('{}debug.csv'.format(path), index=False, encoding='utf-8')\n",
    "train.to_csv('{}train.csv'.format(path), index=False, encoding='utf-8')\n",
    "dev.to_csv('{}dev.csv'.format(path), index=False, encoding='utf-8')\n",
    "test.to_csv('{}test.csv'.format(path), index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wiki talk data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = '../data/wiki_talk/'\n",
    "fname = '{}labeled_data.csv'.format(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv(fname, encoding='utf-8')# , index_col='Unnamed: 0').sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['com']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Toxic'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'toxic'.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total comments: 159571\n",
      "Neutral comments: 143346\n",
      "\t And which country would gain free benifit?\n",
      "\t (I'm lonely and sad.)\n",
      "\t Some pictures for Miranda...\n",
      "\t thank u   u seen rele nice 2 give me a second chance!!\n",
      "\t Talk Talk to me please!\n",
      "Toxic comments: 15294\n",
      "\t fuck u to buddy, i know that was thomas, i aint stupid\n",
      "\t none of us are perfect ok, get off me\n",
      "\t Because you touch yourself at night.\n",
      "\t whoa   you are a big fat idot, stop spamming my userspace\n",
      "\t 2013 (UTC) He is obviously homosexual   17:39, 3 August\n",
      "Severe_Toxic comments: 1595\n",
      "\t FUCK YOU   FUCK YOU ASSHOLE\n",
      "\t regarding you being an asshole go fuck your mother.\n",
      "\t fuck you faggot  fuck you faggot\n",
      "\t fucK of ff f f f f f f f f\n",
      "\t FUCK YOU ALL!   GO TO HELL!\n",
      "Obscene comments: 8449\n",
      "\t Fuck you Juliancolton.\n",
      "\t Ill photo shop a dick in his illuminati mouth\n",
      "\t Go fuck yourself traitor. Thank you.\n",
      "\t Fuck you asshole. Allahu Akbar.\n",
      "\t deleting good articles fuck bag\n",
      "Threat comments: 478\n",
      "\t Sitush is an asshole...die you dog\n",
      "\t Loganberry   i will kill u first\n",
      "\t I like you   I kill you last.\n",
      "\t Fine.  I will destroy you.   Xchanter\n",
      "\t Hello, I think you are a fuckwad. Eat shit and die?\n",
      "Insult comments: 7877\n",
      "\t Look up my damn idiots troll.\n",
      "\t F-U-C-K OFF SLANT EYE CHINK\n",
      "\t ya'll stupid muthafuckas need to mind ya'll own business\n",
      "\t ITS NOT A PERSONAL ATTACK HES MY BRO STOP BEING A HOE\n",
      "\t IN THE ASS BITCH FUCKING COCK SUCKER!\n",
      "Identity_Hate comments: 1405\n",
      "\t ONOREM IS A NAZI FAGGOT\n",
      "\t I'm not the racist. Europeans and Arabs are the racists.\n",
      "\t Fuck you, unpopular opinion, you disgusting faggot.\n",
      "\t hey   way to support nazis, you racist\n",
      "\t U R A FUCKIN MUSLIM ARENT U?\n"
     ]
    }
   ],
   "source": [
    "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "neutral_msk = np.array([True]*len(df))\n",
    "for l in labels:\n",
    "    neutral_msk = neutral_msk & ~np.array([*map(bool, df[l])])\n",
    "msk2 = [len(text) < 60 for text in df['comment_text']]\n",
    "\n",
    "# Print total\n",
    "print(\"Total comments: {}\".format(len(df)))\n",
    "\n",
    "# Print neutral\n",
    "print(\"Neutral comments: {}\".format(sum(neutral_msk)))\n",
    "msk = [m1 and m2 for m1, m2 in zip(neutral_msk, msk2)]\n",
    "for ex in df[msk]['comment_text'].sample(5):\n",
    "    print('\\t', ex.replace('\\n', ' '))\n",
    "\n",
    "for l in labels:\n",
    "    print(\"{} comments: {}\".format(l.title(), sum(df[l])))\n",
    "    msk1 = [*map(bool, df[l])]\n",
    "    msk = [m1 and m2 for m1, m2 in zip(msk1, msk2)]\n",
    "    for ex in df[msk]['comment_text'].sample(5):\n",
    "        print('\\t', ex.replace('\\n', ' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Looking at number of hatebase terms in our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Counter([b for a in df2['slurs'] for b in a]).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Counter([b for a in df['slurs'] for b in a]).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
