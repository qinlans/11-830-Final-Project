{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Local imports\n",
    "from preprocessing import clean_tweets, one_hot_encode, make_debug_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First load and clean the hatebase terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Clean the hatebase slurs\n",
    "def clean_slurs(text):\n",
    "    \"\"\"Lowercase and underscore join slur words.\"\"\"\n",
    "    return text.strip().lower().replace(' ', '_')\n",
    "\n",
    "slurs = pd.read_csv('data/original_hatebase_slurs.txt', header=None)\n",
    "\n",
    "# Clean slurs\n",
    "slur_list = [*map(lambda s: s.lower(), slurs[0].values)]\n",
    "cleaned_slurs = [*map(clean_slurs, slur_list)]\n",
    "pluralize_slurs = [s + end for s in cleaned_slurs for end in ['s', 'es']]\n",
    "full_slur_list = sorted(pluralize_slurs + cleaned_slurs)\n",
    "\n",
    "# Outputs\n",
    "slur_map = {s: cs for s, cs in zip(slur_list, cleaned_slurs) if s != cs}\n",
    "out_slurs = pd.DataFrame(full_slur_list)\n",
    "out_slurs.to_csv('data/hatebase_slurs.txt', index=None, header=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_slurs_in_context(text):\n",
    "    \"\"\"Replace slurs with their cleaned versions.\"\"\"\n",
    "    for k, v in slur_map.items():\n",
    "        text = text.replace(k, v)\n",
    "    return text\n",
    "\n",
    "def extract_slurs(text):\n",
    "    \"\"\"Get a list of all slurs used in the text.\"\"\"\n",
    "    text = text.split(' ')\n",
    "    all_slurs = []\n",
    "    for s in full_slur_list:\n",
    "        if s in text:\n",
    "            all_slurs += [s]\n",
    "    return all_slurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Davidson et al data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = 'data/davidson/'\n",
    "fname = '{}labeled_data.csv'.format(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv(fname, encoding='utf-8', index_col='Unnamed: 0').sample(frac=1)\n",
    "label_map = {0: 'hate_speech', 1: 'offensive_language', 2: 'neither'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clean the tweets\n",
    "df = clean_tweets(df)\n",
    "\n",
    "# Convert columns to one hot encoding\n",
    "df[['hate_speech', 'offensive_language', 'neither']] = \\\n",
    "    one_hot_encode(df['class'])\n",
    "df['label'] =df['class'].apply(lambda c: label_map[c])\n",
    "    \n",
    "# Clean hate speech terms, and extract slurs\n",
    "df['tweet'] = df['tweet'].apply(clean_slurs_in_context)\n",
    "df['slurs'] = df['tweet'].apply(extract_slurs)\n",
    "    \n",
    "# Re-order the DataFrame, and drop some columns\n",
    "df = df[['tweet', 'label', 'mentions', 'hashtags', 'slurs', 'original_tweet',\n",
    "         'hate_speech', 'offensive_language', 'neither']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make a test/dev/train split\n",
    "train_perc = 0.80\n",
    "msk = np.random.rand(len(df)) < train_perc\n",
    "train = df[msk]\n",
    "not_train = df[~msk]\n",
    "half = int(len(not_train) / 2)\n",
    "dev = not_train[:half]\n",
    "test = not_train[half:]\n",
    "\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "debug = make_debug_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "debug.to_csv('{}debug.csv'.format(path), index=False, encoding='utf-8')\n",
    "train.to_csv('{}train.csv'.format(path), index=False, encoding='utf-8')\n",
    "dev.to_csv('{}dev.csv'.format(path), index=False, encoding='utf-8')\n",
    "test.to_csv('{}test.csv'.format(path), index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Zeerak data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/zeerak_naacl/'\n",
    "fname = '{}zeerak_naacl_tweets.csv'.format(path)\n",
    "# Load the data\n",
    "df2 = pd.read_csv(fname, encoding='utf-8').sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First mask out some missing data\n",
    "msk = ~ df2['label'].apply(lambda t: type(t) is float)\n",
    "df2 = df2[msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Clean the tweets\n",
    "df2.rename(index=str, columns={'text': 'tweet'}, inplace=True)\n",
    "df2 = clean_tweets(df2)\n",
    "\n",
    "# Label cleanup to match the other df format\n",
    "labels = ['racism', 'sexism', 'none']\n",
    "one_hot_label = [labels.index(l) for l in df2['label']]\n",
    "for l in labels:\n",
    "    df2[l] = -1\n",
    "\n",
    "# Convert columns to one hot encoding\n",
    "df2[['racism', 'sexism', 'none']] = \\\n",
    "    one_hot_encode(one_hot_label)\n",
    "    \n",
    "# Clean hate speech terms, and extract slurs\n",
    "df2['tweet'] = df2['tweet'].apply(clean_slurs_in_context)\n",
    "df2['slurs'] = df2['tweet'].apply(extract_slurs)\n",
    "    \n",
    "# Re-order the DataFrame, and drop some columns\n",
    "df2 = df2[['tweet', 'label', 'mentions', 'hashtags', 'slurs', 'original_tweet',\n",
    "           'racism', 'sexism', 'none', 'tweet_id', 'user_screen_name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make a test/dev/train split\n",
    "train_perc = 0.80\n",
    "msk = np.random.rand(len(df2)) < train_perc\n",
    "train = df2[msk]\n",
    "not_train = df2[~msk]\n",
    "half = int(len(not_train) / 2)\n",
    "dev = not_train[:half]\n",
    "test = not_train[half:]\n",
    "\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "debug = make_debug_df(df2, cols=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "debug.to_csv('{}debug.csv'.format(path), index=False, encoding='utf-8')\n",
    "train.to_csv('{}train.csv'.format(path), index=False, encoding='utf-8')\n",
    "dev.to_csv('{}dev.csv'.format(path), index=False, encoding='utf-8')\n",
    "test.to_csv('{}test.csv'.format(path), index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Looking at number of hatebase terms in our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bitch', 94),\n",
       " ('idiot', 79),\n",
       " ('pancakes', 37),\n",
       " ('idiots', 33),\n",
       " ('cunt', 29),\n",
       " ('bitches', 22),\n",
       " ('eggs', 21),\n",
       " ('apple', 18),\n",
       " ('property', 17),\n",
       " ('trash', 16),\n",
       " ('egg', 14),\n",
       " ('hoes', 12),\n",
       " ('jihadi', 11),\n",
       " ('cunts', 11),\n",
       " ('jihadis', 10),\n",
       " ('fairy', 7),\n",
       " ('bubble', 7),\n",
       " ('pussy', 6),\n",
       " ('charlie', 6),\n",
       " ('retarded', 5),\n",
       " ('pancake', 5),\n",
       " ('tan', 5),\n",
       " ('mock', 5),\n",
       " ('af', 4),\n",
       " ('skinny', 4),\n",
       " ('cracker', 4),\n",
       " ('twat', 4),\n",
       " ('fruit', 3),\n",
       " ('bogan', 3),\n",
       " ('ghosts', 3),\n",
       " ('shade', 3),\n",
       " ('dhimmi', 3),\n",
       " ('shades', 3),\n",
       " ('banana', 3),\n",
       " ('ho', 3),\n",
       " ('birds', 3),\n",
       " ('dhimmis', 3),\n",
       " ('queens', 3),\n",
       " ('ghost', 3),\n",
       " ('bird', 3),\n",
       " ('frog', 3),\n",
       " ('jocks', 2),\n",
       " ('skip', 2),\n",
       " ('uncivilized', 2),\n",
       " ('zebra', 2),\n",
       " ('redneck', 2),\n",
       " ('coloured', 2),\n",
       " ('shines', 2),\n",
       " ('bucks', 2),\n",
       " ('colored', 2),\n",
       " ('shine', 2),\n",
       " ('hoe', 2),\n",
       " ('nigga', 2),\n",
       " ('bananas', 2),\n",
       " ('oriental', 2),\n",
       " ('retards', 2),\n",
       " ('abo', 2),\n",
       " ('queer', 2),\n",
       " ('spikes', 2),\n",
       " ('apes', 1),\n",
       " ('spike', 1),\n",
       " ('monkey', 1),\n",
       " ('faggots', 1),\n",
       " ('peppers', 1),\n",
       " ('slag', 1),\n",
       " ('slope', 1),\n",
       " ('slit', 1),\n",
       " ('abc', 1),\n",
       " ('ann', 1),\n",
       " ('greaseball', 1),\n",
       " ('boo', 1),\n",
       " ('ike', 1),\n",
       " ('skippy', 1),\n",
       " ('ape', 1),\n",
       " ('poms', 1),\n",
       " ('ghettoes', 1),\n",
       " ('mocks', 1),\n",
       " ('queen', 1),\n",
       " ('zips', 1),\n",
       " ('fuzzy', 1),\n",
       " ('snowflakes', 1),\n",
       " ('pommies', 1),\n",
       " ('prod', 1),\n",
       " ('yokels', 1),\n",
       " ('redskins', 1),\n",
       " ('twats', 1),\n",
       " ('jerry', 1),\n",
       " ('yids', 1),\n",
       " ('coconut', 1),\n",
       " ('gin', 1),\n",
       " ('niggers', 1),\n",
       " ('nigger', 1),\n",
       " ('fruits', 1),\n",
       " ('dyke', 1),\n",
       " ('slags', 1),\n",
       " ('bogans', 1),\n",
       " ('cuck', 1),\n",
       " ('conspiracy_theorist', 1),\n",
       " ('faggot', 1),\n",
       " ('nidge', 1),\n",
       " ('dago', 1),\n",
       " ('bubbles', 1),\n",
       " ('yellow', 1)]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([b for a in df2['slurs'] for b in a]).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bitch', 7851),\n",
       " ('hoes', 4458),\n",
       " ('bitches', 2982),\n",
       " ('pussy', 2056),\n",
       " ('hoe', 1823),\n",
       " ('nigga', 1115),\n",
       " ('trash', 1010),\n",
       " ('niggas', 733),\n",
       " ('faggot', 427),\n",
       " ('bird', 371),\n",
       " ('charlie', 283),\n",
       " ('retarded', 260),\n",
       " ('ghetto', 259),\n",
       " ('niggah', 253),\n",
       " ('yellow', 244),\n",
       " ('yankees', 230),\n",
       " ('cunt', 229),\n",
       " ('nigger', 225),\n",
       " ('fag', 220),\n",
       " ('birds', 219),\n",
       " ('ho', 187),\n",
       " ('colored', 174),\n",
       " ('nicca', 160),\n",
       " ('monkey', 153),\n",
       " ('niccas', 111),\n",
       " ('retard', 108),\n",
       " ('faggots', 108),\n",
       " ('nig', 101),\n",
       " ('nigguh', 94),\n",
       " ('white_trash', 94),\n",
       " ('niggers', 88),\n",
       " ('brownies', 87),\n",
       " ('redneck', 86),\n",
       " ('af', 85),\n",
       " ('queer', 83),\n",
       " ('mock', 78),\n",
       " ('dyke', 72),\n",
       " ('fags', 67),\n",
       " ('crackers', 65),\n",
       " ('jihadi', 64),\n",
       " ('oreo', 60),\n",
       " ('cracker', 60),\n",
       " ('yankee', 59),\n",
       " ('oreos', 54),\n",
       " ('tranny', 49),\n",
       " ('redskins', 49),\n",
       " ('coon', 48),\n",
       " ('teabagger', 46),\n",
       " ('sole', 46),\n",
       " ('skinny', 46),\n",
       " ('teabaggers', 46),\n",
       " ('twat', 44),\n",
       " ('cunts', 43),\n",
       " ('jihadis', 41),\n",
       " ('slope', 40),\n",
       " ('brownie', 39),\n",
       " ('retards', 39),\n",
       " ('hillbilly', 34),\n",
       " ('fuzzy', 34),\n",
       " ('niggahs', 32),\n",
       " ('shy', 31),\n",
       " ('nips', 30),\n",
       " ('whitey', 29),\n",
       " ('coons', 28),\n",
       " ('chug', 27),\n",
       " ('slit', 27),\n",
       " ('uncle_tom', 27),\n",
       " ('apes', 26),\n",
       " ('nigs', 26),\n",
       " ('clam', 25),\n",
       " ('ape', 25),\n",
       " ('rednecks', 25),\n",
       " ('nip', 24),\n",
       " ('soles', 24),\n",
       " ('fairy', 24),\n",
       " ('hoosiers', 23),\n",
       " ('dykes', 23),\n",
       " ('chunky', 22),\n",
       " ('abo', 22),\n",
       " ('jig', 22),\n",
       " ('mocks', 22),\n",
       " ('crow', 22),\n",
       " ('yanks', 22),\n",
       " ('beaner', 22),\n",
       " ('cripple', 22),\n",
       " ('albino', 21),\n",
       " ('boo', 21),\n",
       " ('zebra', 21),\n",
       " ('wop', 19),\n",
       " ('hick', 18),\n",
       " ('queen', 18),\n",
       " ('apple', 18),\n",
       " ('scally', 17),\n",
       " ('hos', 17),\n",
       " ('gook', 17),\n",
       " ('idiot', 16),\n",
       " ('spic', 16),\n",
       " ('wigga', 15),\n",
       " ('chink', 15),\n",
       " ('slant', 15),\n",
       " ('guinea', 14),\n",
       " ('twinkie', 14),\n",
       " ('mickey', 14),\n",
       " ('hun', 14),\n",
       " ('fob', 14),\n",
       " ('muzzie', 13),\n",
       " ('twinkies', 13),\n",
       " ('spook', 13),\n",
       " ('ike', 13),\n",
       " ('pollo', 13),\n",
       " ('oriental', 13),\n",
       " ('honkey', 13),\n",
       " ('queers', 12),\n",
       " ('coloured', 12),\n",
       " ('beaners', 12),\n",
       " ('amos', 11),\n",
       " ('egg', 11),\n",
       " ('clams', 11),\n",
       " ('honky', 11),\n",
       " ('crows', 11),\n",
       " ('shine', 10),\n",
       " ('fruit', 10),\n",
       " ('wetback', 10),\n",
       " ('twats', 10),\n",
       " ('pom', 10),\n",
       " ('wigger', 9),\n",
       " ('anglo', 9),\n",
       " ('chinks', 9),\n",
       " ('slopes', 9),\n",
       " ('zebras', 9),\n",
       " ('wasp', 9),\n",
       " ('niglet', 9),\n",
       " ('nigglet', 9),\n",
       " ('half_breed', 9),\n",
       " ('jigg', 8),\n",
       " ('buck', 8),\n",
       " ('boon', 8),\n",
       " ('shade', 8),\n",
       " ('trailer_trash', 8),\n",
       " ('hicks', 8),\n",
       " ('honkies', 8),\n",
       " ('mook', 8),\n",
       " ('tan', 8),\n",
       " ('ned', 8),\n",
       " ('eggplant', 8),\n",
       " ('yellow_bone', 8),\n",
       " ('shiner', 8),\n",
       " ('ghost', 8),\n",
       " ('shylock', 7),\n",
       " ('porch_monkey', 7),\n",
       " ('trashes', 7),\n",
       " ('leprechaun', 7),\n",
       " ('kike', 7),\n",
       " ('dink', 7),\n",
       " ('nig_nog', 7),\n",
       " ('banana', 7),\n",
       " ('guala', 7),\n",
       " ('bubble', 7),\n",
       " ('ghettos', 7),\n",
       " ('jiggaboo', 7),\n",
       " ('hoser', 7),\n",
       " ('queens', 6),\n",
       " ('wetbacks', 6),\n",
       " ('spear_chucker', 6),\n",
       " ('pinto', 6),\n",
       " ('shades', 6),\n",
       " ('yardie', 6),\n",
       " ('coloreds', 6),\n",
       " ('ling_ling', 6),\n",
       " ('ofay', 6),\n",
       " ('peckerwood', 6),\n",
       " ('yokel', 6),\n",
       " ('eggs', 6),\n",
       " ('honkie', 6),\n",
       " ('nigguhs', 5),\n",
       " ('moor', 5),\n",
       " ('papoose', 5),\n",
       " ('injuns', 5),\n",
       " ('pancakes', 5),\n",
       " ('redskin', 5),\n",
       " ('tiger', 5),\n",
       " ('poms', 5),\n",
       " ('race_traitor', 5),\n",
       " ('wiggers', 5),\n",
       " ('blaxican', 5),\n",
       " ('red_bone', 5),\n",
       " ('ann', 5),\n",
       " ('darkie', 5),\n",
       " ('coon_ass', 5),\n",
       " ('bucks', 5),\n",
       " ('muzzies', 5),\n",
       " ('septic', 5),\n",
       " ('pepsi', 5),\n",
       " ('mutt', 5),\n",
       " ('bubbles', 4),\n",
       " ('apples', 4),\n",
       " ('paki', 4),\n",
       " ('skip', 4),\n",
       " ('jigga', 4),\n",
       " ('rubes', 4),\n",
       " ('yellows', 4),\n",
       " ('jerry', 4),\n",
       " ('squinty', 4),\n",
       " ('spooks', 4),\n",
       " ('tigers', 4),\n",
       " ('mick', 4),\n",
       " ('zip', 4),\n",
       " ('peckerwoods', 4),\n",
       " ('anchor_baby', 4),\n",
       " ('idiots', 4),\n",
       " ('bengali', 4),\n",
       " ('gator_bait', 4),\n",
       " ('pepper', 4),\n",
       " ('spics', 4),\n",
       " ('celestial', 4),\n",
       " ('bananas', 4),\n",
       " ('tinker', 4),\n",
       " ('red_bones', 4),\n",
       " ('coconut', 4),\n",
       " ('property', 4),\n",
       " ('tommy', 4),\n",
       " ('slits', 3),\n",
       " ('bumblebee', 3),\n",
       " ('bumblebees', 3),\n",
       " ('trannys', 3),\n",
       " ('uncle_toms', 3),\n",
       " ('skippy', 3),\n",
       " ('mong', 3),\n",
       " ('whiggers', 3),\n",
       " ('camel_jockey', 3),\n",
       " ('bong', 3),\n",
       " ('trailer_park_trash', 3),\n",
       " ('teapot', 3),\n",
       " ('domes', 3),\n",
       " ('wiggas', 3),\n",
       " ('house_nigger', 3),\n",
       " ('kikes', 3),\n",
       " ('uncivilized', 3),\n",
       " ('moke', 3),\n",
       " ('peppers', 3),\n",
       " ('charlies', 3),\n",
       " ('uncivilised', 3),\n",
       " ('amo', 3),\n",
       " ('gin', 3),\n",
       " ('ginger', 3),\n",
       " ('yank', 3),\n",
       " ('cotton_picker', 3),\n",
       " ('darkies', 3),\n",
       " ('border_jumpers', 3),\n",
       " ('mack', 3),\n",
       " ('pussys', 3),\n",
       " ('pancake', 3),\n",
       " ('nig_nogs', 3),\n",
       " ('border_hopper', 2),\n",
       " ('half_breeds', 2),\n",
       " ('coolie', 2),\n",
       " ('whigger', 2),\n",
       " ('towel_heads', 2),\n",
       " ('jigaboo', 2),\n",
       " ('slants', 2),\n",
       " ('mooks', 2),\n",
       " ('frog', 2),\n",
       " ('sub_human', 2),\n",
       " ('spear_chuckers', 2),\n",
       " ('yardies', 2),\n",
       " ('guido', 2),\n",
       " ('cripples', 2),\n",
       " ('chav', 2),\n",
       " ('bint', 2),\n",
       " ('cushite', 2),\n",
       " ('bung', 2),\n",
       " ('punjab', 2),\n",
       " ('abc', 2),\n",
       " ('raghead', 2),\n",
       " ('chugs', 2),\n",
       " ('hoosier', 2),\n",
       " ('leprechauns', 2),\n",
       " ('white_chocolate', 2),\n",
       " ('injun', 2),\n",
       " ('heeb', 2),\n",
       " ('eurotrash', 2),\n",
       " ('blockhead', 2),\n",
       " ('spike', 2),\n",
       " ('japs', 2),\n",
       " ('ching_chong', 2),\n",
       " ('wink', 2),\n",
       " ('zog', 2),\n",
       " ('jungle_bunny', 2),\n",
       " ('cowboy_killer', 2),\n",
       " ('mud_shark', 2),\n",
       " ('ginzos', 2),\n",
       " ('gyp', 2),\n",
       " ('kushite', 2),\n",
       " ('monkeys', 2),\n",
       " ('niglets', 2),\n",
       " ('greaser', 2),\n",
       " ('leb', 2),\n",
       " ('taffy', 2),\n",
       " ('albinos', 1),\n",
       " ('shines', 1),\n",
       " ('nigglets', 1),\n",
       " ('kraut', 1),\n",
       " ('slag', 1),\n",
       " ('rube', 1),\n",
       " ('huns', 1),\n",
       " ('nigor', 1),\n",
       " ('greaseball', 1),\n",
       " ('wog', 1),\n",
       " ('blockheads', 1),\n",
       " ('niger', 1),\n",
       " ('towel_head', 1),\n",
       " ('tar_baby', 1),\n",
       " ('aunt_jane', 1),\n",
       " ('niggur', 1),\n",
       " ('booner', 1),\n",
       " ('gypos', 1),\n",
       " ('wops', 1),\n",
       " ('cocoa_puff', 1),\n",
       " ('moulie', 1),\n",
       " ('skag', 1),\n",
       " ('island_nigger', 1),\n",
       " ('ginzo', 1),\n",
       " ('muk', 1),\n",
       " ('niggress', 1),\n",
       " ('border_bunny', 1),\n",
       " ('sideways_vagina', 1),\n",
       " ('wic', 1),\n",
       " ('gingers', 1),\n",
       " ('pogue', 1),\n",
       " ('jock', 1),\n",
       " ('border_jumper', 1),\n",
       " ('tinkers', 1),\n",
       " ('race_traitors', 1),\n",
       " ('cocoa_puffs', 1),\n",
       " ('pollos', 1),\n",
       " ('moors', 1),\n",
       " ('yokels', 1),\n",
       " ('moxy', 1),\n",
       " ('niggar', 1),\n",
       " ('piker', 1),\n",
       " ('aunt_jemima', 1),\n",
       " ('darky', 1),\n",
       " ('ping_pang', 1),\n",
       " ('cowboy_killers', 1),\n",
       " ('scanger', 1),\n",
       " ('aunt_mary', 1),\n",
       " ('mutts', 1),\n",
       " ('steek', 1),\n",
       " ('moon_cricket', 1),\n",
       " ('mangia_cake', 1),\n",
       " ('yob', 1),\n",
       " ('nigra', 1),\n",
       " ('black_barbie', 1),\n",
       " ('timber_nigger', 1),\n",
       " ('shylocks', 1),\n",
       " ('eggplants', 1),\n",
       " ('redlegs', 1),\n",
       " ('prairie_nigger', 1),\n",
       " ('slant_eye', 1),\n",
       " ('scallies', 1),\n",
       " ('tunnel_digger', 1),\n",
       " ('hunkie', 1),\n",
       " ('guidos', 1),\n",
       " ('heinie', 1),\n",
       " ('jigaboos', 1),\n",
       " ('fruits', 1),\n",
       " ('buckwheat', 1),\n",
       " ('bunga', 1),\n",
       " ('armo', 1),\n",
       " ('argie', 1),\n",
       " ('hosers', 1),\n",
       " ('jiggas', 1),\n",
       " ('nacho', 1),\n",
       " ('ragheads', 1),\n",
       " ('yid', 1),\n",
       " ('gooks', 1),\n",
       " ('squaw', 1),\n",
       " ('bitter_clingers', 1),\n",
       " ('mtn', 1),\n",
       " ('poppadom', 1),\n",
       " ('zipperheads', 1),\n",
       " ('fuzzy_wuzzy', 1),\n",
       " ('boonies', 1),\n",
       " ('jants', 1),\n",
       " ('hillbillys', 1),\n",
       " ('wagon_burner', 1),\n",
       " ('bitchs', 1),\n",
       " ('pancake_face', 1),\n",
       " ('jap', 1),\n",
       " ('dives', 1),\n",
       " ('prod', 1),\n",
       " ('jiggers', 1),\n",
       " ('nachos', 1),\n",
       " ('closet_fag', 1),\n",
       " ('wexican', 1),\n",
       " ('chaves', 1),\n",
       " ('shit_heel', 1),\n",
       " ('dhimmi', 1),\n",
       " ('winks', 1),\n",
       " ('moulinyan', 1),\n",
       " ('boos', 1),\n",
       " ('dhimmis', 1),\n",
       " ('fez', 1),\n",
       " ('hayseed', 1),\n",
       " ('cotton_pickers', 1),\n",
       " ('wasps', 1),\n",
       " ('hunky', 1),\n",
       " ('gables', 1)]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([b for a in df['slurs'] for b in a]).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
